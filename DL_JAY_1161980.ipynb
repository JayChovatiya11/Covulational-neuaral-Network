{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbEWJJwRNvl7",
        "outputId": "98d29c25-21d2-4cb3-9d22-948ed27da313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 16, 16, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 8, 8, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 4, 4, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               2097664   \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 100)               51300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,708,900\n",
            "Trainable params: 2,706,596\n",
            "Non-trainable params: 2,304\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "390/390 [==============================] - 34s 85ms/step - loss: 4.1049 - accuracy: 0.1147 - val_loss: 4.5149 - val_accuracy: 0.0605\n",
            "Epoch 2/50\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 3.1383 - accuracy: 0.2377 - val_loss: 3.0756 - val_accuracy: 0.2681\n",
            "Epoch 3/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 2.6887 - accuracy: 0.3184 - val_loss: 2.4219 - val_accuracy: 0.3888\n",
            "Epoch 4/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 2.4374 - accuracy: 0.3691 - val_loss: 2.3998 - val_accuracy: 0.3902\n",
            "Epoch 5/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 2.2630 - accuracy: 0.4037 - val_loss: 2.1537 - val_accuracy: 0.4485\n",
            "Epoch 6/50\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 2.1376 - accuracy: 0.4295 - val_loss: 2.0643 - val_accuracy: 0.4633\n",
            "Epoch 7/50\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 2.0386 - accuracy: 0.4516 - val_loss: 1.9483 - val_accuracy: 0.4839\n",
            "Epoch 8/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.9480 - accuracy: 0.4722 - val_loss: 1.7921 - val_accuracy: 0.5220\n",
            "Epoch 9/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.8745 - accuracy: 0.4893 - val_loss: 1.8448 - val_accuracy: 0.5129\n",
            "Epoch 10/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.8186 - accuracy: 0.5015 - val_loss: 1.8409 - val_accuracy: 0.5060\n",
            "Epoch 11/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.7737 - accuracy: 0.5128 - val_loss: 1.7527 - val_accuracy: 0.5295\n",
            "Epoch 12/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.7213 - accuracy: 0.5260 - val_loss: 1.6642 - val_accuracy: 0.5549\n",
            "Epoch 13/50\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 1.6873 - accuracy: 0.5361 - val_loss: 1.8415 - val_accuracy: 0.5233\n",
            "Epoch 14/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.6473 - accuracy: 0.5412 - val_loss: 1.7695 - val_accuracy: 0.5283\n",
            "Epoch 15/50\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 1.6058 - accuracy: 0.5546 - val_loss: 1.6257 - val_accuracy: 0.5644\n",
            "Epoch 16/50\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 1.5689 - accuracy: 0.5605 - val_loss: 1.7404 - val_accuracy: 0.5457\n",
            "Epoch 17/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.5344 - accuracy: 0.5695 - val_loss: 1.5190 - val_accuracy: 0.5879\n",
            "Epoch 18/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.5021 - accuracy: 0.5756 - val_loss: 1.5130 - val_accuracy: 0.5883\n",
            "Epoch 19/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.4762 - accuracy: 0.5847 - val_loss: 1.6308 - val_accuracy: 0.5615\n",
            "Epoch 20/50\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 1.4534 - accuracy: 0.5899 - val_loss: 1.6055 - val_accuracy: 0.5716\n",
            "Epoch 21/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.4246 - accuracy: 0.5937 - val_loss: 1.5987 - val_accuracy: 0.5714\n",
            "Epoch 22/50\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 1.4082 - accuracy: 0.5982 - val_loss: 1.5131 - val_accuracy: 0.5830\n",
            "Epoch 23/50\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 1.3871 - accuracy: 0.6057 - val_loss: 1.5479 - val_accuracy: 0.5890\n",
            "Epoch 24/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.3680 - accuracy: 0.6068 - val_loss: 1.5439 - val_accuracy: 0.5892\n",
            "Epoch 25/50\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 1.3534 - accuracy: 0.6157 - val_loss: 1.5131 - val_accuracy: 0.5892\n",
            "Epoch 26/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.3308 - accuracy: 0.6196 - val_loss: 1.4856 - val_accuracy: 0.5983\n",
            "Epoch 27/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.3135 - accuracy: 0.6219 - val_loss: 1.6566 - val_accuracy: 0.5726\n",
            "Epoch 28/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.3027 - accuracy: 0.6244 - val_loss: 1.3916 - val_accuracy: 0.6185\n",
            "Epoch 29/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.2808 - accuracy: 0.6313 - val_loss: 1.4014 - val_accuracy: 0.6188\n",
            "Epoch 30/50\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.2693 - accuracy: 0.6326 - val_loss: 1.4805 - val_accuracy: 0.6017\n",
            "Epoch 31/50\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 1.2562 - accuracy: 0.6377 - val_loss: 1.3719 - val_accuracy: 0.6275\n",
            "Epoch 32/50\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 1.2407 - accuracy: 0.6404 - val_loss: 1.3745 - val_accuracy: 0.6233\n",
            "Epoch 33/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.2322 - accuracy: 0.6444 - val_loss: 1.4259 - val_accuracy: 0.6181\n",
            "Epoch 34/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.2114 - accuracy: 0.6462 - val_loss: 1.4662 - val_accuracy: 0.6050\n",
            "Epoch 35/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.2095 - accuracy: 0.6499 - val_loss: 1.3848 - val_accuracy: 0.6231\n",
            "Epoch 36/50\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 1.1908 - accuracy: 0.6532 - val_loss: 1.3698 - val_accuracy: 0.6262\n",
            "Epoch 37/50\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 1.1745 - accuracy: 0.6566 - val_loss: 1.4350 - val_accuracy: 0.6184\n",
            "Epoch 38/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1722 - accuracy: 0.6585 - val_loss: 1.3867 - val_accuracy: 0.6254\n",
            "Epoch 39/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.1573 - accuracy: 0.6622 - val_loss: 1.4181 - val_accuracy: 0.6153\n",
            "Epoch 40/50\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.1543 - accuracy: 0.6625 - val_loss: 1.4274 - val_accuracy: 0.6193\n",
            "Epoch 41/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.1327 - accuracy: 0.6681 - val_loss: 1.3477 - val_accuracy: 0.6317\n",
            "Epoch 42/50\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 1.1231 - accuracy: 0.6724 - val_loss: 1.3899 - val_accuracy: 0.6235\n",
            "Epoch 43/50\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 1.1163 - accuracy: 0.6719 - val_loss: 1.3911 - val_accuracy: 0.6253\n",
            "Epoch 44/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1125 - accuracy: 0.6708 - val_loss: 1.4383 - val_accuracy: 0.6227\n",
            "Epoch 45/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.0998 - accuracy: 0.6776 - val_loss: 1.3222 - val_accuracy: 0.6361\n",
            "Epoch 46/50\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 1.1015 - accuracy: 0.6749 - val_loss: 1.3493 - val_accuracy: 0.6335\n",
            "Epoch 47/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.0839 - accuracy: 0.6808 - val_loss: 1.3408 - val_accuracy: 0.6355\n",
            "Epoch 48/50\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 1.0827 - accuracy: 0.6785 - val_loss: 1.4015 - val_accuracy: 0.6322\n",
            "Epoch 49/50\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.0635 - accuracy: 0.6844 - val_loss: 1.3968 - val_accuracy: 0.6235\n",
            "Epoch 50/50\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 1.0587 - accuracy: 0.6848 - val_loss: 1.3258 - val_accuracy: 0.6426\n",
            "Test Accuracy: 64.26%\n"
          ]
        }
      ],
      "source": [
        "#importing the libraries\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization,Dropout\n",
        "from keras.datasets import cifar100\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "# first convolutional layer\n",
        "model.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', input_shape=(32, 32, 3))) \n",
        "model.add(BatchNormalization())\n",
        "# second convolutional layer\n",
        "model.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) \n",
        "model.add(Dropout(0.25))\n",
        "# third convolutional layer\n",
        "model.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "# fourth convolutional layer\n",
        "model.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) \n",
        "model.add(Dropout(0.25))\n",
        "# fifth convolutional layer\n",
        "model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten()) \n",
        "#fully connected layer\n",
        "model.add(Dense(512, activation='relu')) \n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='softmax')) \n",
        "# printing the model summary\n",
        "model.summary()\n",
        "# loading the cifar100 data\n",
        "(cifarx_train, cifary_train), (cifarx_test, cifary_test) = cifar100.load_data()\n",
        "# normalize the pixel values \n",
        "cifarx_train = cifarx_train.astype('float32') / 255\n",
        "cifarx_test = cifarx_test.astype('float32') / 255\n",
        "# convert the labels to one-hot encoded vectors\n",
        "cifary_train = np_utils.to_categorical(cifary_train, 100)\n",
        "cifary_test = np_utils.to_categorical(cifary_test, 100)\n",
        "# define different learning rates for different convolutional layers\n",
        "lr_schedule = {0: 0.01,1: 0.0002, 2: 0.001,3: 0.0005,4: 0.0001}\n",
        "# define the adam optimizer\n",
        "optimizer = Adam(lr=lr_schedule[0])\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# define data augmentation parameters\n",
        "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1,horizontal_flip=True)\n",
        "# Using Adam and set learning rate 0.001\n",
        "optimizer = Adam(lr=0.001)  \n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# train the model\n",
        "history = model.fit(datagen.flow(cifarx_train,cifary_train, batch_size=128, shuffle=True), \n",
        "                    steps_per_epoch=len(cifarx_train)/128, epochs=50, validation_data=(cifarx_test, cifary_test))\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(cifarx_test, cifary_test, verbose=0)\n",
        "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# Define plotchart function\n",
        "def plotchart(history, value):\n",
        "    plt.figure(figsize=[8,6])\n",
        "    plt.plot(history.history['loss'], 'firebrick', linewidth=3.0)\n",
        "    plt.plot(history.history['accuracy'], 'turquoise', linewidth=3.0)\n",
        "    plt.legend(['Training loss', 'Training Accuracy'], fontsize=18)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss and Accuracy', fontsize=16)\n",
        "    plt.title('Loss and Accuracy Curves for {}'.format(value), fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plotchart(history, 'CIFAR-100 image classification task')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
